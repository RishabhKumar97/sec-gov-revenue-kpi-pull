{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090a61ec-b16e-4b3e-bb01-2d965486a93e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0283d2eb-5a65-4a93-854e-75b09b5a32f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "143f1cb6-9947-41c4-a0df-53dcb08a3252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "header = {'User-Agent' : \"rkumar11@mscience.com\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74339ee-8d22-4259-81de-49ddc03ee776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Input tickers which needs to be checked for their quarterly/yearly total revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c85dbc-3b32-4c04-b576-0abde880338f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tickers_str_with_region = \"\"\"\n",
    "FL-US\n",
    "LULU-US\n",
    "LE-US\n",
    "KIRK-US\n",
    "CTRN-US\n",
    "GES-US\n",
    "GCO-US\n",
    "PETS-US\n",
    "DKS-US\n",
    "RH-US\n",
    "CBRL-US\n",
    "CAL-US\n",
    "VSCO-US\n",
    "AEO-US\n",
    "ZUMZ-US\n",
    "BKE-US\n",
    "DBI-US\n",
    "SFIX-US\n",
    "GME-US\n",
    "PETS-US\n",
    "PLAY-US\n",
    "UNFI-US\n",
    "ASO-US\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182eb544-b89c-443d-bca4-c462eaa1d7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ouput Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ed6e8c9-e6d4-4de3-9b2c-f473165f5ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Output list of tickers 1.when revenue is not available 2.In cases where any exception occurs(delisted etc)\n",
    "rev_bif_not_found = []\n",
    "\n",
    "#processed tickers\n",
    "processed_tickers = []\n",
    "\n",
    "#Final result df with the required schema\n",
    "final_result_df = spark.createDataFrame([], schema = StructType([StructField('Ticker', StringType(), False), StructField('Period', StringType(), False), StructField('Period_Startdate', StringType(), True), StructField('Period_Enddate', StringType(), True), StructField('Period_Reportdate', StringType(), True), StructField('_', StringType(), False), StructField('Segment_Identifier', StringType(), False), StructField('KPI', StringType(), False), StructField('Value', StringType(), True), StructField('Revison_Date', StringType(), False), StructField('Original_Value', StringType(), False), StructField('Source', StringType(), False)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b98f416-6b44-4f86-9e39-aa80e9af7c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a4a4227-6456-44b5-9015-0f41b0ff44a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_input_tickers(tickers_str_with_region):\n",
    "  #segregate tickers by region\n",
    "  tickers_list_us_region = [ticker for ticker in tickers_str_with_region.split(\"\\n\") if ticker != \"\" and ticker.lstrip().rstrip().endswith(\"-US\")]\n",
    "  tickers_list_int_region = [ticker for ticker in tickers_str_with_region.split(\"\\n\") if ticker != \"\" and not ticker.lstrip().rstrip().endswith(\"-US\")]\n",
    "\n",
    "  #get the ticker part only\n",
    "  tickers_list = [ticker.split(\"-\")[0] for ticker in tickers_list_us_region]\n",
    "\n",
    "  #handle cases with duplicate entries\n",
    "  tickers_to_check =  [ticker for ticker in set(tickers_list)]\n",
    "\n",
    "  #add international tickers to flag\n",
    "  rev_bif_not_found.extend(list(set(tickers_list_int_region)))\n",
    "  return tickers_to_check\n",
    "\n",
    "def get_cik_lookup_df():\n",
    "  #Get CIK lookup table through company tickers endpoint\n",
    "  cik_map = requests.get(\"https://www.sec.gov/files/company_tickers.json\", headers = header)\n",
    "  cik_map_df = spark.createDataFrame(cik_map.json().values(), schema = StructType([StructField('cik_str', StringType(), True),\\\n",
    "                                                                                            StructField('ticker', StringType(), True),\\\n",
    "                                                                                            StructField('title', StringType(), True)]))\n",
    "  #padding to get 10 digit cik \n",
    "  cik_map_df = cik_map_df.withColumn('cik_str', F.lpad(F.col('cik_str'), 10, '0'))\n",
    "  return cik_map_df\n",
    "\n",
    "def get_company_facts(cik):\n",
    "    #company facts endpoint\n",
    "    URL = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    time.sleep(1)\n",
    "    comp_facts = requests.get(URL, headers = header)\n",
    "    comp_facts_dict = json.loads(comp_facts.text)\n",
    "    return comp_facts_dict\n",
    "\n",
    "def choose_best_fin_statement(comp_facts_dict, Ticker):\n",
    "    rev_reporting_statements = [\"RevenueFromContractWithCustomerExcludingAssessedTax\", \"RevenueFromContractWithCustomerIncludingAssessedTax\", \"Revenues\"]\n",
    "    potential_keys = []\n",
    "    try:\n",
    "      for each in comp_facts_dict['facts']['us-gaap'].keys():\n",
    "        if(each in rev_reporting_statements):\n",
    "          potential_keys.append(each)\n",
    "      #case when the correct statements are not there\n",
    "      if(len(potential_keys) == 0):\n",
    "        print(f\"No matching revenues statements found for {Ticker}\")\n",
    "        rev_bif_not_found.append(Ticker + \"-US\")\n",
    "        return None\n",
    "      elif(len(potential_keys)>1):\n",
    "        #choosing the best financials statement for tracking revenue out of the available\n",
    "        df_key_max_date_and_rev = {}\n",
    "        for k in potential_keys:\n",
    "          df = spark.createDataFrame(comp_facts_dict['facts']['us-gaap'][k]['units']['USD'])\\\n",
    "                        .withColumn('end', F.col('end').cast(DateType()))\\\n",
    "                        .withColumn('val', F.col('val').cast(LongType()))\n",
    "          max_date = df.agg(F.max(F.col('end')).alias('max_date')).first()['max_date']\n",
    "          max_val = df.filter(F.col('end') == max_date).first()['val'] \n",
    "          df_key_max_date_and_rev[k] = (max_date, max_val)\n",
    "        # Sort by max_date first, then by revenue(we know Total revenue will always be >= net sales/any other revenue KPI)\n",
    "        key_to_use = sorted(df_key_max_date_and_rev.items(), key=lambda x: (x[1][0], x[1][1]), reverse=True)[0][0]\n",
    "      else:\n",
    "        key_to_use = potential_keys[0]\n",
    "    #raise exception in case us-gaap facts are not available\n",
    "    except KeyError:\n",
    "      print(f\"us-gaap financials not available for {Ticker}\")\n",
    "      rev_bif_not_found.append(Ticker + \"-US\")\n",
    "      return None\n",
    "    return key_to_use\n",
    "\n",
    "def create_df_and_run_transformations(comp_facts_dict, key_to_use):\n",
    "    #create df for the company facts\n",
    "    flat_facts_df = spark.createDataFrame(comp_facts_dict['facts']['us-gaap'][key_to_use]['units']['USD'])\n",
    "\n",
    "    # drop columns that are not required\n",
    "    flat_facts_df = flat_facts_df.drop(*['description', 'label', 'accn', 'frame', 'fy'])\n",
    "    \n",
    "    #transformations\n",
    "    flat_facts_df = flat_facts_df.withColumn('start', F.col('start').cast(DateType()))\\\n",
    "                                .withColumn('filed', F.col('filed').cast(DateType()))\\\n",
    "                                .withColumn('end', F.col('end').cast(DateType()))\\\n",
    "                                .withColumn('val_int', F.col('val').cast(LongType()))\\\n",
    "                                .withColumn(\"time_period_months\", F.round(F.months_between(F.col('end'), F.col('start')), 2))\\\n",
    "                                .withColumn(\"start_year\", F.year(F.date_trunc('year', F.col('start'))).cast(StringType()))\n",
    "                                \n",
    "    flat_facts_df = (flat_facts_df\n",
    "                      #filtering out records that are not within the last 2 years --edit here for more ticker history --IMPORTANT\n",
    "                      .filter(\"start > date_sub(current_date(), 732) \")\n",
    "                      #filter out rows having running total revenue instead of quarter/yearly values \n",
    "                      .filter(\"  (time_period_months between 2.7 and 4) or (time_period_months between 11 and 13) \")\n",
    "    )\n",
    "\n",
    "    # drop duplicate rows   \n",
    "    flat_facts_df = flat_facts_df.dropDuplicates(['start', 'end', 'val_int'])\n",
    "    # print(flat_facts_df.schema)\n",
    "    return flat_facts_df\n",
    "\n",
    "def get_fiscal_year_quarters(flat_facts_df, Ticker, assign_leniency_to_quarter_end_in_days= 23, gen_prev_num_quarters= 16, gen_next_num_quarters= 9):\n",
    "    #select relevant columns\n",
    "    all_data_df = flat_facts_df.select('start', 'end', 'fp', 'time_period_months') \n",
    "    # display(all_data_df)\n",
    "\n",
    "    #get latest fy enddate\n",
    "    fiscal_year_end = all_data_df.filter(\"\"\" time_period_months between 11 and 13 \"\"\").select(F.max(F.col('end'))).collect()[0][0]\n",
    "    # print(\"fiscal_year_end\", fiscal_year_end)\n",
    "    if(fiscal_year_end is None):\n",
    "        print(f\"No FY rows found for {Ticker} and hence quarter assignment not possible for this ticker\")\n",
    "        processed_tickers.append(Ticker + \"-US\")\n",
    "        return None\n",
    "    else:\n",
    "      #idea is to get q1-q3 using the latest fiscal year enddate by subtracting/adding 3 months consecutively e.g.subtracting 3 months from the current FY enddate will give the enddate for 3Q, repeating that again will give 2Q enddate\n",
    "      quarters_dict ={\n",
    "      '1Q' : [],\n",
    "      '2Q' : [],\n",
    "      '3Q' : []\n",
    "      }\n",
    "      \n",
    "      fiscal_year_end_m = fiscal_year_end\n",
    "      fiscal_year_end_p = fiscal_year_end\n",
    "      \n",
    "      #generating quarter mapping for dates that are lower than the latest FY end (considering prev 15 quarters in this case as per default argument)\n",
    "      for i in range(1, gen_prev_num_quarters, 1):\n",
    "          fiscal_year_end_m = fiscal_year_end_m - relativedelta(months=3)\n",
    "          if(i==1 or (i-1)%4==0):\n",
    "              quarters_dict['3Q'].append(fiscal_year_end_m)\n",
    "          elif(i==2 or (i-2)%4==0):\n",
    "              quarters_dict['2Q'].append(fiscal_year_end_m)\n",
    "          elif(i==3 or (i+1)%4==0):\n",
    "              quarters_dict['1Q'].append(fiscal_year_end_m)\n",
    "\n",
    "      #generating quarter mapping for dates that are greater than the latest FY end (considering next 8 quarters in this case as per default argument)\n",
    "      for i in range(1, gen_next_num_quarters, 1):\n",
    "          fiscal_year_end_p = fiscal_year_end_p + relativedelta(months=3)\n",
    "          if(i==1 or (i-1)%4==0):\n",
    "              quarters_dict['1Q'].append(fiscal_year_end_p)\n",
    "          elif(i==2 or (i-2)%4==0):\n",
    "              quarters_dict['2Q'].append(fiscal_year_end_p)\n",
    "          elif(i==3 or (i+1)%4==0):\n",
    "              quarters_dict['3Q'].append(fiscal_year_end_p)\n",
    "      \n",
    "      quarters_df = spark.createDataFrame(data = [quarters_dict])\n",
    "      \n",
    "      quarters_df = quarters_df.withColumn(\"1Q\", F.explode(F.col(\"1Q\")))\\\n",
    "                          .withColumn(\"2Q\", F.explode(F.col(\"2Q\")))\\\n",
    "                          .withColumn(\"3Q\", F.explode(F.col(\"3Q\")))\n",
    "      # print(quarters_dict)\n",
    "      #floor and ceil for the quarter's enddate\n",
    "      q1_df = quarters_df.select('1Q').distinct().withColumn('1Q_floor', F.date_sub(F.col('1Q'), assign_leniency_to_quarter_end_in_days))\\\n",
    "                                                .withColumn('1Q_ceil', F.date_add(F.col('1Q'), assign_leniency_to_quarter_end_in_days))\n",
    "      # display(q1_df)\n",
    "      q2_df = quarters_df.select('2Q').distinct().withColumn('2Q_floor', F.date_sub(F.col('2Q'), assign_leniency_to_quarter_end_in_days))\\\n",
    "                                                .withColumn('2Q_ceil', F.date_add(F.col('2Q'), assign_leniency_to_quarter_end_in_days))\n",
    "\n",
    "      q3_df = quarters_df.select('3Q').distinct().withColumn('3Q_floor', F.date_sub(F.col('3Q'), assign_leniency_to_quarter_end_in_days))\\\n",
    "                                                .withColumn('3Q_ceil', F.date_add(F.col('3Q'), assign_leniency_to_quarter_end_in_days))\n",
    "      \n",
    "      #consider only periods with 3 months for assigning fp i.e. q1, q2, q3(FY and q4 will be handled later)\n",
    "      all_data_df_not_FY  = all_data_df.filter(\"not(time_period_months between 11 and 13)\")\n",
    "      \n",
    "      #assigning quarters(i.e. fiscal period (fp)) based on floor and ceil range\n",
    "      q1_assigned = all_data_df_not_FY.crossJoin(q1_df).withColumn('fp', F.when((F.col('end')>=F.col('1Q_floor')) & (F.col('end')<=F.col('1Q_ceil')), F.lit('1Q')).otherwise(F.lit('')))\n",
    "      # display(q1_assigned)\n",
    "      \n",
    "      q1_q2_assigned = q1_assigned.crossJoin(q2_df).withColumn('fp', F.when((F.col('end')>=F.col('2Q_floor')) & (F.col('end')<=F.col('2Q_ceil')), F.lit('2Q')).otherwise(F.col('fp')))\n",
    "      \n",
    "      q1_q2_q3_assigned = q1_q2_assigned.crossJoin(q3_df).withColumn('fp', F.when((F.col('end')>=F.col('3Q_floor')) & (F.col('end')<=F.col('3Q_ceil')), F.lit('3Q')).otherwise(F.col('fp')))\n",
    "\n",
    "      quarters_q1_to_q3_alloted = q1_q2_q3_assigned.filter(F.col('fp') != '').dropDuplicates(['start', 'end', 'fp', 'time_period_months']).withColumn('fp_alloted', F.col('fp')).select(['start', 'end', 'fp_alloted', 'time_period_months'])\n",
    "      \n",
    "      #return the df with alloted quarters\n",
    "      return quarters_q1_to_q3_alloted\n",
    "\n",
    "def assign_quarters_to_fiscal_year(all_periods_alloted):\n",
    "    #This function establishes relationship between quarters and the fiscal year i.e. which quarters belong to which fiscal year .This would be useful for q4 rows calculations\n",
    "\n",
    "    #get all fiscal year rows in the data available for the ticker\n",
    "    FY_df = all_periods_alloted.filter(F.col('time_period_months').between(11, 13)).selectExpr('start as Fy_start', 'end as Fy_end')\n",
    "    # display(FY_df)\n",
    "\n",
    "    #latest FY end\n",
    "    latest_fy_end = FY_df.select(F.max(F.col('Fy_end'))).collect()[0][0]\n",
    "    \n",
    "    #assign quarters to the correct fiscal year based on the period range\n",
    "    periods_alloted_with_fy_fp_relation = all_periods_alloted.crossJoin(FY_df).withColumn('fy_fp_relation', F.when((F.col('start') >= F.col('Fy_start')) & (F.col('end') <= F.col('Fy_end')), F.concat_ws(' ', F.col('Fy_start'), F.col('Fy_end'))).otherwise(''))\n",
    "\n",
    "    periods_alloted_with_fy_fp_relation = periods_alloted_with_fy_fp_relation.filter(F.col('fy_fp_relation') != '').dropDuplicates(['start', 'end' , 'val_int'])\n",
    "    \n",
    "    #making sure to include the recent quarterly data as it's FY row is not available yet\n",
    "    missing_recent_rows = all_periods_alloted.join(periods_alloted_with_fy_fp_relation, *[all_periods_alloted.columns], 'left_anti')\n",
    "  \n",
    "    missing_recent_rows = missing_recent_rows.filter((F.col('fp_alloted').isin(['1Q', '2Q', '3Q']) & (F.col('end') > latest_fy_end))).withColumn('Fy_start', F.lit('')).withColumn('Fy_end', F.lit('')).withColumn('fy_fp_relation', F.lit(''))\n",
    "\n",
    "    all_periods_alloted_with_fy_fp_relation = periods_alloted_with_fy_fp_relation.union(missing_recent_rows.select(*[periods_alloted_with_fy_fp_relation.columns]))\n",
    "\n",
    "    all_periods_alloted_with_fy_fp_relation = all_periods_alloted_with_fy_fp_relation.drop(*['Fy_start', 'Fy_end'])\n",
    "    # all_periods_alloted_with_fy_fp_relation.display()\n",
    "    return all_periods_alloted_with_fy_fp_relation\n",
    "\n",
    "def get_q4_rows(all_periods_alloted_with_fy_fp_relation):\n",
    "    #calcs below for q4 rows if not present in the original df\n",
    "    \n",
    "    #get values comprised of q1,q2, q3 and FY\n",
    "    q1_q4_fy_grp_df = all_periods_alloted_with_fy_fp_relation.groupBy('fy_fp_relation').agg(F.max('val_int').alias('max_val_per_grp'),\\\n",
    "                                                              F.max('end').alias('fy_end'),\\\n",
    "                                                              F.max('filed').alias('fy_filed_date'),\\\n",
    "                                                              F.count('*').alias('fy_period_cnt')).filter(F.col('fy_period_cnt') == 4)\n",
    "    \n",
    "    q1_q4_fy_grp_list = [row['max_val_per_grp'] for row in q1_q4_fy_grp_df.collect()]\n",
    "    \n",
    "    #get q1-q3 values\n",
    "    q1_q3_grp = all_periods_alloted_with_fy_fp_relation.filter(~F.col('val_int').isin(q1_q4_fy_grp_list))\\\n",
    "                                 .groupBy('fy_fp_relation')\\\n",
    "                                    .agg(F.count('*').alias('quarter_count'),\\\n",
    "                                         F.sum('val_int').alias('sum_q1_q3_val'),\\\n",
    "                                         F.max('end').alias('q3_end_date'))\\\n",
    "                                .filter(F.col('quarter_count') == 3)\n",
    "    # calc q4 values\n",
    "    q4_df = q1_q4_fy_grp_df.join(q1_q3_grp, 'fy_fp_relation', 'inner')\\\n",
    "                     .withColumn('q4_val', F.col('max_val_per_grp') - F.col('sum_q1_q3_val'))\\\n",
    "                     .withColumn('q4_start_date', F.date_add(F.col('q3_end_date'), 1))\\\n",
    "                     .withColumn('Period', F.concat_ws('', F.lit('4Q'), F.regexp_extract(F.col('fy_fp_relation'), '^\\\\d{2}(\\\\d{2})[-]', 1)))\n",
    "    \n",
    "    #match the schema with the original df for union after return\n",
    "    q4_df = q4_df.withColumn('start', F.col('q4_start_date'))\\\n",
    "                 .withColumn('end', F.col('fy_end') )\\\n",
    "                 .withColumn('time_period_months', F.lit(3))\\\n",
    "                 .withColumn('val', F.col('q4_val').cast(StringType()))\\\n",
    "                 .withColumn('form', F.lit(' '))\\\n",
    "                 .withColumn('filed', F.col('fy_filed_date'))\\\n",
    "                 .withColumn('val_int', F.col('q4_val').cast(LongType()))\\\n",
    "                 .withColumn('end_year', F.lit(' ') )\\\n",
    "                 .withColumn('fp_alloted', F.lit('4Q'))\\\n",
    "                 .withColumn('Period', F.col('Period'))\\\n",
    "                 .withColumn('start_year', F.year(F.date_trunc('year', F.col('start'))).cast(StringType()))\n",
    "    \n",
    "    return q4_df.select(*[all_periods_alloted_with_fy_fp_relation.columns])\n",
    "\n",
    "def get_base_sheet_format(output_df, Ticker):\n",
    "    #base sheet format\n",
    "    output_df  = output_df.withColumn('Ticker', F.lit(f'{Ticker}' + \" \" + \"US Equity\"))\\\n",
    "                            .withColumn('start_date_string', F.date_format(F.col('start'), 'M/d/y'))\\\n",
    "                            .withColumn('filed_date_string', F.date_format(F.col('filed'), 'M/d/y'))\\\n",
    "                            .withColumn('end_date_string', F.date_format(F.col('end'), 'M/d/y'))\\\n",
    "                            .withColumn('Period_Startdate', F.col('start_date_string'))\\\n",
    "                            .withColumn('Period_Enddate', F.col('end_date_string'))\\\n",
    "                            .withColumn('Period_Reportdate', F.col('filed_date_string'))\\\n",
    "                            .withColumn('Value', F.col('val_int'))\\\n",
    "                            .withColumn('KPI', F.lit('rev_Topline'))\\\n",
    "                            .withColumn('_', F.lit(''))\\\n",
    "                            .withColumn('Segment_Identifier', F.lit(''))\\\n",
    "                            .withColumn('Revison_Date', F.lit(''))\\\n",
    "                            .withColumn('Original_Value', F.lit(''))\\\n",
    "                            .withColumn('Source', F.concat_ws(' ', 'form', 'Period'))\\\n",
    "                            .withColumn(\"value\", F.format_number(F.col(\"value\").cast(LongType()), \"###,###\"))\n",
    "\n",
    "    output_df = output_df.orderBy(F.col('end').desc(), F.col('val_int').desc())\\\n",
    "                         .select(['Ticker', 'Period', 'Period_Startdate', 'Period_Enddate', 'Period_Reportdate', '_', 'Segment_Identifier',  'KPI', 'Value' , 'Revison_Date', 'Original_Value' , 'Source'])\\\n",
    "                          .filter((F.col('Period').rlike('Q|F')) & (F.col('val_int') > 0 ))\n",
    "    return output_df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c0d801d-97ff-4728-ba3e-3ebb03392305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check the input tickers for their cik and title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec647729-df66-4bb2-9de1-8bc8d3c3a2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cik_map_df = get_cik_lookup_df().filter(F.col('ticker').isin(get_input_tickers(tickers_str_with_region)))\n",
    "display(cik_map_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0961e8e0-c64f-4852-9c7e-64b7b1c290af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Main loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a9b2bd2-363f-4fab-8e49-a5120a3c78af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for Ticker in get_input_tickers(tickers_str_with_region):\n",
    "    try:\n",
    "        cik_map_df_filt = cik_map_df.filter(F.col('ticker') == Ticker)\n",
    "        cik = cik_map_df_filt.select('cik_str').collect()[0][0]\n",
    "    except IndexError:\n",
    "        print(f\"{Ticker} not present in cik map. Check cmd 10 for more info\")\n",
    "        rev_bif_not_found.append(Ticker + \"-US\")\n",
    "        continue\n",
    "    print(f\"Processing ticker {Ticker}, cik {cik}\")\n",
    "\n",
    "    comp_facts_dict = get_company_facts(cik)\n",
    "\n",
    "    key_to_use = choose_best_fin_statement(comp_facts_dict, Ticker)\n",
    "    if(key_to_use is None):\n",
    "        continue\n",
    "    else:\n",
    "        flat_facts_df = create_df_and_run_transformations(comp_facts_dict, key_to_use)\n",
    "\n",
    "        #get correct q1-q3 quarters through function call\n",
    "        q1_q3_fn_return = get_fiscal_year_quarters(flat_facts_df, Ticker)\n",
    "\n",
    "        if(q1_q3_fn_return is None):\n",
    "             continue\n",
    "        else:\n",
    "            quarters_q1_to_q3_alloted = flat_facts_df.join(q1_q3_fn_return, ['start', 'end', 'time_period_months'], 'left')\n",
    "\n",
    "            #allot fy and q4(if any)\n",
    "            all_periods_alloted = quarters_q1_to_q3_alloted.drop('fp').withColumn('fp_alloted', F.when(F.col('fp_alloted').isNull() & (F.col('time_period_months').between(2.7, 4)), F.lit('4Q')).otherwise(F.col('fp_alloted'))).withColumn('fp_alloted', F.when(F.col('fp_alloted').isNull() & (F.col('time_period_months').between(11, 13)), F.lit('FY')).otherwise(F.col('fp_alloted'))).filter(F.col('fp_alloted').isNotNull())\n",
    "            \n",
    "            #assign quarters to their respective fiscal_years\n",
    "            all_periods_alloted_with_fy_fp_relation = assign_quarters_to_fiscal_year(all_periods_alloted)\n",
    "\n",
    "            # assign period values e.g. 1Q22, FY21 etc\n",
    "            all_periods_alloted_with_fy_fp_relation = all_periods_alloted_with_fy_fp_relation.withColumn('Period', F.when(F.col('fy_fp_relation') == '', F.concat_ws('', F.col('fp_alloted'), F.regexp_extract(F.col('start_year'), '^\\\\d{2}(\\\\d{2})$', 1)))\\\n",
    "                                                                            .otherwise(F.concat_ws('', F.col('fp_alloted'), F.regexp_extract(F.col('fy_fp_relation'), '^\\\\d{2}(\\\\d{2})[-]', 1))))\n",
    "\n",
    "            #handling cases with amended revenue values --try to optimize\n",
    "            amend_rev_rows = all_periods_alloted_with_fy_fp_relation.groupBy(['start', 'end', 'time_period_months']).agg(F.max('filed').alias('latest_rep_date'), F.count('*').alias('cnt_rows_with_amend')).filter(F.col('cnt_rows_with_amend') > 1).select(['*']).collect()\n",
    "            \n",
    "            for row in amend_rev_rows:\n",
    "                Start = row['start']\n",
    "                End = row['end']\n",
    "                latest_rep_date = row['latest_rep_date']\n",
    "                \n",
    "                #filter out original rows that were amended at a later date\n",
    "                all_periods_alloted_with_fy_fp_relation  = all_periods_alloted_with_fy_fp_relation.filter(~((F.col('start') == Start) & (F.col('end') == End) & (F.col('filed') < latest_rep_date)))\n",
    "            \n",
    "\n",
    "            #call function to get missing 4th quarter rows and union \n",
    "            output_df = all_periods_alloted_with_fy_fp_relation.union(get_q4_rows(all_periods_alloted_with_fy_fp_relation))\n",
    "\n",
    "            #call function to get the df converted into the base sheet format\n",
    "            output_df = get_base_sheet_format(output_df, Ticker)\n",
    "            \n",
    "            #union with final result\n",
    "            final_result_df = final_result_df.unionByName(output_df) \n",
    "\n",
    "            print(f\"Finished processing {Ticker}\")\n",
    "            #add to processed tickers\n",
    "            processed_tickers.append(Ticker + \"-US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78e1869-0240-40b1-b626-c9e036493971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccdc09b2-225c-4478-bee1-eccc08dcbe94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91988b50-d85d-437b-951b-f2758edfd7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check rows that needs to be added in the base sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e292ed03-0120-4172-99f2-dd0ff06d9793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "not_in_base_sheet = (\n",
    "    F.broadcast(final_result_df.withColumn('period_sub_str', F.col('period').substr(0, 2)).alias('a'))\n",
    "    .join(\n",
    "     table('bronze.internal_products.financials_cians_raw')\n",
    "     .filter(F.col('kpi') == \"rev_Topline\")\n",
    "     .select('ticker', 'period', 'period_startdate', 'period_enddate', 'period_reportdate', 'kpi', 'value')\n",
    "     .withColumn('period_sub_str', F.col('period').substr(0, 2))\n",
    "     .alias('b'), \n",
    "          on= (F.col(\"a.Ticker\") == F.col(\"b.ticker\")) & (F.col(\"a.Period_Startdate\") == F.col(\"b.period_startdate\")) & (F.col(\"a.Period_Enddate\") == F.col(\"b.period_enddate\")) & (F.col(\"a.period_sub_str\") == F.col(\"b.period_sub_str\")),\n",
    "          how= 'left_anti')\n",
    "    .drop('period_sub_str')\n",
    ")\n",
    "display(not_in_base_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e8782d-9550-4a44-b65f-a64bde0a80ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "add_to_base_sheet = [row['Ticker'].replace(\" US Equity\", \"-US\") for row in not_in_base_sheet.select('Ticker').distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c2c7923-7305-4944-a4b9-671302fad9de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "QA to check if api was hit for all US tickers"
    }
   },
   "outputs": [],
   "source": [
    "new_processed_tickers = [t.replace(\"-US\", \"\") for t in processed_tickers]\n",
    "new_rev_bif_not_found = [t.replace(\"-US\", \"\") for t in rev_bif_not_found]\n",
    "\n",
    "new_processed_tickers.extend(new_rev_bif_not_found)\n",
    "\n",
    "set(get_input_tickers(tickers_str_with_region)) == set(new_processed_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3718fd42-251a-4a18-a233-f87f30a57bfd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate ticker string for next run"
    }
   },
   "outputs": [],
   "source": [
    "print(('\\n').join(set(processed_tickers) - set(add_to_base_sheet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa3d8b1-2e58-41ec-bce4-da558888f833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exit notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b961eb26-3127-4ce5-b5b1-08acfa561ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(f\"\\nFinancials for the following tickers were not found: {rev_bif_not_found}\\nFollowing tickers were processed: {processed_tickers}\\nNew rows for these tickers can be added: {add_to_base_sheet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1be7119-ca68-44f7-9385-4caecb98edce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **END**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d9c7cba-de77-40a5-9421-602d28ff8a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final_result_df.write.mode('overwrite').saveAsTable('silver_dev.dse_investigations.sec_gov_api_rev_bench_daily_workflow_tickers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "123a4399-7418-4bab-b9a6-90324fb42d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(table('silver_dev.dse_investigations.sec_gov_api_rev_bench_daily_workflow_tickers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bad2f09-26be-49ae-9bb1-7c98c67734c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(table('bronze.internal_products.financials_cians_raw').schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f706c5a3-fdbb-4071-98f1-675ca650a7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8991b2f-b1ef-4e7d-9a6c-4bed96243f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "display(\n",
    "    all_periods_alloted_with_fy_fp_relation.withColumn('fy_plus_q1_q3_rev', F.sum(F.col('val_int')).over(Window.partitionBy(F.col('fy_fp_relation'))))\n",
    "                                          .withColumn('q1_q3_rev', F.when(F.col('fp_alloted') == \"FY\" , F.col('fy_plus_q1_q3_rev') - F.col('val_int')).otherwise(None))\n",
    "                                          .withColumn('q4_rev', F.when(F.col('fp_alloted') == \"FY\" , F.col('val_int') - F.col('q1_q3_rev')).otherwise(None))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3ab2db4-777e-4fd2-86e8-5d4df5221252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(potential_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9df6ccb8-36c4-4c6e-8e3d-15fd04c9ae9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_key_max_date_and_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33681e37-87c3-458d-a342-248a8cece899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(key_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1005ae31-3775-44bb-96b7-507b517a7683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df = (spark.createDataFrame(json_normalize(comp_facts_dict['facts']['us-gaap']['RevenueFromContractWithCustomerExcludingAssessedTax']['units']))\n",
    "        # .withColumn('USD', F.explode(F.col('USD')))\n",
    "        # .select(\"USD.*\")\n",
    "        # .withColumn('start', F.col('start').cast(DateType()))\n",
    "        # .withColumn('end', F.col('end').cast(DateType()))\n",
    "        # .withColumn(\"time_period_months\", F.round(F.months_between(F.col('end'), F.col('start')), 2))\n",
    "        # .filter(\"start > '2018-01-01' \")\n",
    "        # .filter(\"\"\" time_period_months between 2.7 and 4 or time_period_months between 11 and 13  \"\"\")\n",
    "        # # .filter(F.col('time_period_months').between(11.50, 13))\n",
    "        # .dropDuplicates(['start', 'end', 'val'])\n",
    "        # .drop(*['description', 'label', 'accn', 'frame', 'fy'])\n",
    "        # .orderBy(F.col('end').desc()))\n",
    ")\n",
    "\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eefde21f-7318-4764-996d-2284ad0353c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_df.join(get_fiscal_year_quarters(test_df), ['start', 'end', 'time_period_months'], 'left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "389d5d7a-394a-4403-8bad-79b22976347c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.createDataFrame(json_normalize(comp_facts_dict['facts']['us-gaap']['Revenues']['units'])).withColumn('USD', F.explode(F.col('USD'))).select(\"USD.*\").withColumn('end', F.col('end').cast(DateType())).orderBy(F.col('end').desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95f1b66-9909-4b3a-9ca8-8a9fe2b6a0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(flat_facts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "961c9ee9-0f11-4aa7-b7e8-4c5e25ebc767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(quarters_q1_to_q3_alloted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6376dbc5-e563-496f-8a73-0df16011fda2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_fiscal_year_quarters(flat_facts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3b70615-2ad0-400a-ba30-818d4109ae42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(q1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3973df4-dad2-48dd-aff0-fcc9d639af1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(all_periods_alloted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d99d2ae-0e23-4659-a9a6-f53d81f48d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print([each for each in comp_facts_dict['facts']['us-gaap'].keys() if re.search(\"revenue\", each.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224cf1f9-dd43-4f2f-99a8-c6d980f1c8ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(comp_facts_dict['facts']['us-gaap'][key_to_use]['units']['USD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3502fa2-7a22-4780-b61d-24776576bc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "potential_keys = []\n",
    "\n",
    "for each in comp_facts_dict['facts']['us-gaap'].keys():\n",
    "    if(each == \"RevenueFromContractWithCustomerExcludingAssessedTax\" or each == \"Revenues\"):\n",
    "        potential_keys.append(each)\n",
    "        \n",
    "print(potential_keys)\n",
    "\n",
    "if(len(potential_keys)>1):\n",
    "    df_key_count = {}\n",
    "    for k in potential_keys:\n",
    "        df_key_count[k] = spark.createDataFrame(pd.json_normalize(comp_facts_dict['facts']['us-gaap'][k]['units'])).withColumn('USD', F.explode(F.col('USD'))).select(\"USD.*\").filter(F.col('form').isin(['10-K', '10-Q'])).count()\n",
    "\n",
    "key_to_use = max(df_key_count, key=df_key_count.get)\n",
    "print(key_to_use)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbdaeff8-a9b2-4596-9395-af03f07c229d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "key_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7caeb4c-1c7f-4851-8756-aca8f0d90c51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(comp_facts_dict['facts']['us-gaap'][key_to_use]['units']['USD'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sec_gov_revenue_KPI_pull_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
