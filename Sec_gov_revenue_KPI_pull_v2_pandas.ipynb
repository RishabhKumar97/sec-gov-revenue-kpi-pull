{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86e1926-1991-412c-9727-73ae95b06799",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c675cbd-38f6-4517-bb56-cecd4b24e4eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pyspark imports"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaba3146-8f53-4b60-8090-5b43f4b4775e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widgets config"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(name=\"include_historical_data\", defaultValue=\"False\", choices=[\"True\", \"False\"])\n",
    "dbutils.widgets.text(name = \"history_years_param\", defaultValue=\"3\")\n",
    "\n",
    "history_years = eval(dbutils.widgets.get(\"history_years_param\"))\n",
    "include_historical_data = eval(dbutils.widgets.get(\"include_historical_data\"))\n",
    "print(\"history_years: \", history_years)\n",
    "print(\"include_historical_data: \", include_historical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed22a0d0-1da5-4626-8ac1-718fbee0dede",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Logger config"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(name= os.path.basename(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()))\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "\n",
    "formatter = logging.Formatter(fmt= \"{asctime} - {name} - {levelname} --> {message}\", style= '{', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "logger.handlers.clear()    \n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(\"Notebook run started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab228f86-b379-49be-ab93-4e61975b5171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "header = {'User-Agent' : \"rkumar11@mscience.com\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dba1f186-e7db-47b9-8ecb-abc56fab782c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Input tickers which needs to be checked for their quarterly/yearly total revenues"
    }
   },
   "outputs": [],
   "source": [
    "tickers_str_with_region = \"\"\" \n",
    "RRR-US\n",
    "UPWK-US\n",
    "DPZ-US\n",
    "SBUX-US\n",
    "BLMN-US\n",
    "CHWY-US\n",
    "DRVN-US\n",
    "TEAM-US\n",
    "AMZN-US\n",
    "WIX-US\n",
    "ORLY-US\n",
    "BABA-US\n",
    "PETS-US\n",
    "ALK-US\n",
    "OGS-US\n",
    "RGS-US\n",
    "EA-US\n",
    "KMX-US\n",
    "PDD-US\n",
    "RAVE-US\n",
    "IDT-US\n",
    "GT-US\n",
    "NKE-US\n",
    "ULCC-US\n",
    "KR-US\n",
    "FDX-US\n",
    "BJRI-US\n",
    "DTE-US\n",
    "ACI-US\n",
    "LOCO-US\n",
    "TSCO-US\n",
    "ADBE-US\n",
    "BYD-US\n",
    "ODP-US\n",
    "SFIX-US\n",
    "SSTK-US\n",
    "CAKE-US\n",
    "DECK-US\n",
    "PGR-US\n",
    "EXR-US\n",
    "T-US\n",
    "WW-US\n",
    "COUR-US\n",
    "UPS-US\n",
    "MNSO-US\n",
    "GDDY-US\n",
    "FVRR-US\n",
    "EWCZ-US\n",
    "BOOT-US\n",
    "SKT-US\n",
    "LOVE-US\n",
    "PBPB-US\n",
    "CUBE-US\n",
    "VG-US\n",
    "NTES-US\n",
    "MNDY-US\n",
    "AAPL-US\n",
    "CCL-US\n",
    "PLAY-US\n",
    "UNFI-US\n",
    "TALK-US\n",
    "COIN-US\n",
    "BBBY-US\n",
    "CHTR-US\n",
    "CVX-US\n",
    "XOM-US\n",
    "GWW-US\n",
    "PETS-US\n",
    "DENN-US\n",
    "SPG-US\n",
    "OGS-US\n",
    "MED-US\n",
    "TDUP-US\n",
    "UPWK-US\n",
    "BRCC-US\n",
    "HIMS-US\n",
    "SGHC-US\n",
    "FUBO-US\n",
    "IAC-US\n",
    "BRK.A-US\n",
    "NSA-US\n",
    "GT-US\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6cc7598-cced-4b9d-bdfe-9bd74ae6277d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ouput Variables"
    }
   },
   "outputs": [],
   "source": [
    "# Output list of tickers 1.when revenue is not available 2.In cases where any exception occurs(delisted etc)\n",
    "rev_bif_not_found = []\n",
    "\n",
    "# processed tickers\n",
    "processed_tickers = []\n",
    "\n",
    "# Final result df with the required schema\n",
    "final_result_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Ticker\",\n",
    "        \"Period\",\n",
    "        \"Period_Startdate\",\n",
    "        \"Period_Enddate\",\n",
    "        \"Period_Reportdate\",\n",
    "        \"_\",\n",
    "        \"Segment_Identifier\",\n",
    "        \"KPI\",\n",
    "        \"Value\",\n",
    "        \"Revison_Date\",\n",
    "        \"Original_Value\",\n",
    "        \"Source\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5960ab8b-724f-47d3-b931-8516bee063b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Decorator to invoke the corresponding pyspark function"
    }
   },
   "outputs": [],
   "source": [
    "class InvokeCorrespondingDeco:\n",
    "    _map_funcs = {\n",
    "                \"get_input_tickers\" : \"get_input_tickers_pyspark\",\n",
    "                \"get_cik_lookup_df\" : \"get_cik_lookup_df_pyspark\",\n",
    "                \"get_company_facts\" : \"get_company_facts_pyspark\",\n",
    "                \"create_stmnts_combined_df\" : \"create_stmnts_combined_df_pyspark\",\n",
    "                \"remove_inconsistencies\" : \"remove_inconsistencies_pyspark\",\n",
    "                \"save_latest_q1_q3\" : \"save_latest_q1_q3_pyspark\",\n",
    "                \"assign_relation_between_q_and_fy\" : \"assign_relation_between_q_and_fy_pyspark\",\n",
    "                \"get_q4_rows\" : \"get_q4_rows_pyspark\",\n",
    "                \"get_fy_all_q_union_df\" : \"get_fy_all_q_union_df_pyspark\",\n",
    "                \"period_corrected\" : \"period_corrected_pyspark\",\n",
    "                \"get_base_sheet_format\" : \"get_base_sheet_format_pyspark\",\n",
    "                \"get_not_in_base_sheet_rows\"   : \"get_not_in_base_sheet_rows_pyspark\",\n",
    "            }\n",
    "    \n",
    "    __DECORATOR_LOGS = {k:{\"only_in_pandas_df\": None, \"only_in_pyspark_df\": None} for k, v in _map_funcs.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def invoke_corresponding(func):\n",
    "\n",
    "        def wrap(*args, **kwargs):\n",
    "            # map functions functions to pyspark functions\n",
    "            \n",
    "            print(f'Comparing the pandas function \"{func.__name__}\" against the pyspark function \"{InvokeCorrespondingDeco._map_funcs[func.__name__]}\" ')\n",
    "\n",
    "            pandas_output = func(*args, **kwargs)\n",
    "            pandas_output_copy = pandas_output.copy()\n",
    "            \n",
    "            #create args and keyword args for the corresponding pyspark function\n",
    "            pyspark_args = [spark.createDataFrame(arg) if isinstance(arg, pd.DataFrame) else arg for arg in args]\n",
    "            pyspark_kwargs = {k: spark.createDataFrame(v) if isinstance(v, pd.DataFrame) else v for k, v in kwargs.items()}\n",
    "\n",
    "            #calling the corresponding pyspark function\n",
    "            pyspark_output = globals()[InvokeCorrespondingDeco._map_funcs[func.__name__]](*pyspark_args, **pyspark_kwargs)\n",
    "            \n",
    "            if isinstance(pandas_output, pd.DataFrame):\n",
    "                cols_only_in_pandas = set(pandas_output.columns) - set(pyspark_output.columns)\n",
    "                cols_only_in_pyspark = set(pyspark_output.columns) - set(pandas_output.columns)\n",
    "                if cols_only_in_pandas or cols_only_in_pyspark:\n",
    "                    print(f\"Schema difference between found between pandas and pyspark output\")\n",
    "                    print(f\"Columns only in pandas output: {cols_only_in_pandas}\")\n",
    "                    print(f\"Columns only in pyspark output: {cols_only_in_pyspark}\")\n",
    "                else:\n",
    "                    print(\"Schema is same between pandas and pyspark output\")\n",
    "                merge_keys =  [col for col in pyspark_output.columns if re.search(col.lower(), \"start|end\")]\n",
    "                pyspark_output_pd = pyspark_output.toPandas()\n",
    "\n",
    "                if pandas_output_copy.empty:\n",
    "                    print(\"pandas df is empty\")\n",
    "                elif pyspark_output.isEmpty():\n",
    "                    print(\"pyspark df is empty\")\n",
    "                elif pandas_output_copy.empty and pyspark_output.isEmpty():\n",
    "                    print(\"Both pandas and pyspark df are empty\")\n",
    "                else:\n",
    "                    joined_df = pd.merge(\n",
    "                        left = pandas_output_copy, \n",
    "                        right = pyspark_output_pd,\n",
    "                        on= merge_keys,\n",
    "                        suffixes=(\"_pandas\", \"_pyspark\"),\n",
    "                        how= \"outer\",\n",
    "                        indicator=True\n",
    "                    )\n",
    "                    InvokeCorrespondingDeco.__DECORATOR_LOGS[func.__name__][\"only_in_pandas_df\"] = joined_df[joined_df[\"_merge\"] == \"left_only\"]\n",
    "                    InvokeCorrespondingDeco.__DECORATOR_LOGS[func.__name__][\"only_in_pyspark_df\"] = joined_df[joined_df[\"_merge\"] == \"right_only\"]\n",
    "\n",
    "            elif isinstance(pandas_output, list) or isinstance(pandas_output, dict):\n",
    "                if not pandas_output == pyspark_output:\n",
    "                    print(\"Pandas and pyspark outputs are different\")\n",
    "                    # print(pandas_output)\n",
    "                    # print(pyspark_output)\n",
    "                else:\n",
    "                    print(\"Pandas and pyspark outputs are same\")\n",
    "            print(\"-\"*150)\n",
    "            return pandas_output\n",
    "        return wrap\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_deco_logs():\n",
    "        return __DECORATOR_LOGS\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65d202ad-6fa5-4679-ba11-a52bca74a687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def debug(func):\n",
    "    def debug_wrap(*args, **kwargs):\n",
    "        print(f\"Calling {func.__name__} with args {args} and kwargs {kwargs}\")\n",
    "        return func(*args, **kwargs)\n",
    "    return debug_wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c094a698-50b1-4fe9-b8e0-e27052ec7473",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pyspark functions"
    }
   },
   "outputs": [],
   "source": [
    "# @debug\n",
    "def get_input_tickers_pyspark(tickers_str_with_region:str) -> list[str]:\n",
    "    # segregate tickers by region\n",
    "    tickers_list_us_region = [\n",
    "        ticker.lstrip().rstrip()\n",
    "        for ticker in tickers_str_with_region.split(\"\\n\")\n",
    "        if ticker != \"\" and ticker.endswith(\"-US\")\n",
    "    ]\n",
    "    # print(tickers_list_us_region)\n",
    "    tickers_list_int_region = [\n",
    "        ticker.lstrip().rstrip()\n",
    "        for ticker in tickers_str_with_region.split(\"\\n\")\n",
    "        if ticker != \"\" and not ticker.endswith(\"-US\")\n",
    "    ]\n",
    "    # print(tickers_list_int_region)\n",
    "    # get the ticker part only\n",
    "    tickers_list = [ticker.split(\"-\")[0] for ticker in tickers_list_us_region]\n",
    "\n",
    "    # handle cases with duplicate entries\n",
    "    tickers_to_check = [ticker for ticker in set(tickers_list)]\n",
    "\n",
    "    # add international tickers to flag\n",
    "    rev_bif_not_found.extend([t for t in list(set(tickers_list_int_region)) if t != \"\"])\n",
    "    # print(rev_bif_not_found)\n",
    "    return tickers_to_check\n",
    "\n",
    "# # @debug\n",
    "def get_cik_lookup_df_pyspark() -> DataFrame:\n",
    "    # Get CIK lookup table through company tickers endpoint\n",
    "    cik_map = requests.get(\n",
    "        \"https://www.sec.gov/files/company_tickers.json\", headers=header\n",
    "    )\n",
    "    # print(cik_map.status_code)\n",
    "    cik_map_df = spark.createDataFrame(\n",
    "        cik_map.json().values(),\n",
    "        schema=StructType(\n",
    "            [\n",
    "                StructField(\"cik_str\", StringType(), True),\n",
    "                StructField(\"ticker\", StringType(), True),\n",
    "                StructField(\"title\", StringType(), True),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    # padding to get 10 digit cik\n",
    "    cik_map_df = cik_map_df.withColumn(\"cik_str\", F.lpad(F.col(\"cik_str\"), 10, \"0\"))\n",
    "    return cik_map_df\n",
    "\n",
    "# @debug\n",
    "def get_company_facts_pyspark(cik:str) -> dict:\n",
    "    # company facts endpoint\n",
    "    URL = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    comp_facts = requests.get(URL, headers=header)\n",
    "    time.sleep(1)\n",
    "    comp_facts_dict = json.loads(comp_facts.text)\n",
    "    return comp_facts_dict\n",
    "\n",
    "# @debug\n",
    "def create_stmnts_combined_df_pyspark(comp_facts_dict: dict, history_years: int= 3):\n",
    "    # stmnts_to_scan= [stmnt for stmnt in comp_facts_dict['facts']['us-gaap'].keys() if re.search(\"revenue\", stmnt.lower())]\n",
    "    if \"us-gaap\" not in comp_facts_dict[\"facts\"].keys():\n",
    "        return None\n",
    "    else:\n",
    "        stmnts_to_scan = [\n",
    "            \"RevenueFromContractWithCustomerExcludingAssessedTax\",\n",
    "            \"RevenueFromContractWithCustomerIncludingAssessedTax\",\n",
    "            # 'RevenueFromContractsWithCustomers',\n",
    "            \"Revenues\",\n",
    "        ]\n",
    "        stmnts_combined_df_op = spark.createDataFrame(\n",
    "            [],\n",
    "            schema=StructType(\n",
    "                [\n",
    "                    StructField(\"end\", StringType(), True),\n",
    "                    StructField(\"fp\", StringType(), True),\n",
    "                    StructField(\"start\", StringType(), True),\n",
    "                    StructField(\"filed\", StringType(), True),\n",
    "                    StructField(\"time_period_months\", FloatType(), True),\n",
    "                    StructField(\"val\", LongType(), True),\n",
    "                    StructField(\"orig_stmnt\", StringType(), True),\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        for stmnt in stmnts_to_scan:\n",
    "            try:\n",
    "                stmnt_df = spark.createDataFrame(\n",
    "                    comp_facts_dict[\"facts\"][\"us-gaap\"][stmnt][\"units\"][\"USD\"]\n",
    "                )\n",
    "                # check must have columns -- if any statements lacks these skip it\n",
    "                if not (\n",
    "                    {\"start\", \"end\", \"fp\", \"val\", \"filed\"}.issubset(\n",
    "                        set(stmnt_df.columns)\n",
    "                    )\n",
    "                ):\n",
    "                    continue\n",
    "                # basic transformations and filtering\n",
    "                else:\n",
    "                    stmnt_df = (\n",
    "                        stmnt_df.withColumn(\"start\", F.col(\"start\").cast(DateType()))\n",
    "                        .withColumn(\"end\", F.col(\"end\").cast(DateType()))\n",
    "                        .withColumn(\"filed\", F.col(\"filed\").cast(DateType()))\n",
    "                        .withColumn(\"val\", F.col(\"val\").cast(LongType()))\n",
    "                        .withColumn(\n",
    "                            \"time_period_months\",\n",
    "                            F.round(F.months_between(F.col(\"end\"), F.col(\"start\")), 2),\n",
    "                        )\n",
    "                        .withColumn(\"orig_stmnt\", F.lit(stmnt))\n",
    "                    )\n",
    "                    stmnt_df = (\n",
    "                        stmnt_df\n",
    "                        # filtering out records that are not within the last history_years --IMPORTANT\n",
    "                        .filter(\n",
    "                            f\" start > date_sub(current_date(), {history_years*366}) \"\n",
    "                        )\n",
    "                        # filter out rows that are not quarterly/yearly\n",
    "                        .filter(\n",
    "                            \"  (time_period_months between 2.7 and 4) or (time_period_months between 11 and 13) \"\n",
    "                        )\n",
    "                    )\n",
    "                    stmnts_combined_df_op = stmnts_combined_df_op.unionByName(\n",
    "                        stmnt_df.select(\n",
    "                            \"start\",\n",
    "                            \"end\",\n",
    "                            \"fp\",\n",
    "                            \"time_period_months\",\n",
    "                            \"val\",\n",
    "                            \"filed\",\n",
    "                            \"orig_stmnt\",\n",
    "                        )\n",
    "                    )\n",
    "            except KeyError as ke:\n",
    "                # logger.error(f\"KeyError: {ke}\")\n",
    "                continue\n",
    "    return stmnts_combined_df_op.select(\n",
    "        \"start\", \"end\", \"fp\", \"time_period_months\", \"val\", \"filed\", \"orig_stmnt\"\n",
    "    )\n",
    "\n",
    "# @debug\n",
    "def remove_inconsistencies_pyspark(stmnts_combined_df: DataFrame) -> DataFrame:\n",
    "    stmnts_combined_df_cleansed = (\n",
    "        stmnts_combined_df.withColumn(\n",
    "            \"max_rev\", F.max(\"val\").over(Window.partitionBy(\"start\", \"end\", \"fp\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"rank_rev\",\n",
    "            F.row_number().over(\n",
    "                Window.partitionBy(\"start\", \"end\", \"fp\", \"max_rev\").orderBy(\n",
    "                    F.desc(\"val\")\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        .filter(F.col(\"rank_rev\") == 1)\n",
    "        .drop(\"max_rev\", \"rank_rev\")\n",
    "        # Filtering out ambigous cases\n",
    "        .filter(\n",
    "            r\"\"\" fp is not null and not(time_period_months < 11 and fp == \"FY\") and not(time_period_months > 11 and fp rlike \"^Q\\\\d{1}$\") \"\"\"\n",
    "        )\n",
    "        .groupBy(\"start\", \"end\", \"time_period_months\", \"fp\")\n",
    "        .agg(\n",
    "            F.count(F.lit(1)).alias(\"count\"),\n",
    "            F.max(\"val\").alias(\"val\"),\n",
    "            F.collect_list(F.col(\"filed\")).alias(\"list_filed\"),\n",
    "        )\n",
    "        # filtering out rows with less count when start and end are same but fp is different\n",
    "        .withColumn(\n",
    "            \"rn\",\n",
    "            F.rank().over(Window.partitionBy(\"start\", \"end\").orderBy(F.desc(\"count\"))),\n",
    "        )\n",
    "        .filter(F.col(\"rn\") == 1)\n",
    "        # filtering out rows with less count when fp is same and the respective start and end dates are very close in terms of days(inconsistent data)\n",
    "        .withColumn(\"start_month\", F.trunc(F.col(\"start\"), \"month\"))\n",
    "        .withColumn(\"end_month\", F.trunc(F.col(\"end\"), \"month\"))\n",
    "        .withColumn(\n",
    "            \"rnk_m\",\n",
    "            F.rank().over(\n",
    "                Window.partitionBy(\"start_month\", \"end_month\", \"fp\").orderBy(\n",
    "                    F.col(\"count\").desc()\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        .filter(F.col(\"rnk_m\") == 1)\n",
    "        .withColumn(\"week_end\", F.date_trunc(\"week\", F.col(\"end\")))\n",
    "        .withColumn(\"week_start\", F.date_trunc(\"week\", F.col(\"start\")))\n",
    "        .withColumn(\n",
    "            \"rnk_w\",\n",
    "            F.rank().over(\n",
    "                Window.partitionBy(\"week_end\", \"week_start\", \"fp\").orderBy(\n",
    "                    F.col(\"count\").desc()\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        .filter(F.col(\"rnk_w\") == 1)\n",
    "        .withColumn(\"filed\", F.explode(\"list_filed\"))\n",
    "        .drop(\n",
    "            \"rn\",\n",
    "            \"rnk_m\",\n",
    "            \"rnk_w\",\n",
    "            \"week_end\",\n",
    "            \"week_start\",\n",
    "            \"start_month\",\n",
    "            \"end_month\",\n",
    "            \"count\",\n",
    "            \"list_filed\",\n",
    "        )\n",
    "    )\n",
    "    return stmnts_combined_df_cleansed\n",
    "\n",
    "# @debug\n",
    "def save_latest_q1_q3_pyspark(stmnts_combined_df_cleansed: DataFrame) -> DataFrame:\n",
    "    # save the latest q1-q3 for union later\n",
    "    max_end_fy = (\n",
    "        stmnts_combined_df_cleansed.filter(\" fp == 'FY' \")\n",
    "        .agg(F.max(\"end\"))\n",
    "        .first()[0]\n",
    "    )\n",
    "    latest_q1_q3 = stmnts_combined_df_cleansed.filter(F.col(\"end\") > max_end_fy)\n",
    "    return latest_q1_q3\n",
    "\n",
    "# @debug\n",
    "def assign_relation_between_q_and_fy_pyspark(stmnts_combined_df_cleansed):\n",
    "    fy_rows = stmnts_combined_df_cleansed.filter(\" fp == 'FY' \").selectExpr(\n",
    "        \"start as fy_start\", \"end as fy_end\"\n",
    "    )\n",
    "\n",
    "    stmnts_combined_df_with_rel = (\n",
    "        stmnts_combined_df_cleansed.crossJoin(fy_rows)\n",
    "        .filter(\" start >= fy_start and end <= fy_end\")\n",
    "        .withColumn(\n",
    "            \"q1_q3_fy_rel\", F.concat_ws(\" \", F.col(\"fy_start\"), F.col(\"fy_end\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return stmnts_combined_df_with_rel\n",
    "\n",
    "# @debug\n",
    "def get_q4_rows_pyspark(stmnts_combined_df_with_rel: DataFrame) -> DataFrame:\n",
    "    q4_rows_df = (\n",
    "        stmnts_combined_df_with_rel.withColumn(\n",
    "            \"q1_q3_fy_rev\", F.sum(\"val\").over(Window.partitionBy(\"q1_q3_fy_rel\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"q1_q3_rev\",\n",
    "            F.when(F.col(\"fp\") == \"FY\", F.col(\"q1_q3_fy_rev\") - F.col(\"val\")).otherwise(\n",
    "                None\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"q4_rev\",\n",
    "            F.when(F.col(\"fp\") == \"FY\", F.col(\"val\") - F.col(\"q1_q3_rev\")).otherwise(\n",
    "                None\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\"q4_rev\", F.max(\"q4_rev\").over(Window.partitionBy(\"q1_q3_fy_rel\")))\n",
    "        .withColumn(\n",
    "            \"rnk_q1_q3_fy_rel\",\n",
    "            F.rank().over(\n",
    "                Window.partitionBy(\"q1_q3_fy_rel\").orderBy(F.col(\"end\").desc())\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"q4_start\",\n",
    "            F.when(\n",
    "                F.col(\"rnk_q1_q3_fy_rel\") == 2, F.date_add(F.col(\"end\"), 1)\n",
    "            ).otherwise(None),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"q4_end\",\n",
    "            F.when(F.col(\"rnk_q1_q3_fy_rel\") == 1, F.col(\"end\")).otherwise(None),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"q4_filed\",\n",
    "            F.when(F.col(\"rnk_q1_q3_fy_rel\") == 1, F.col(\"filed\")).otherwise(None),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"running_max_q4_start\",\n",
    "            F.max(F.col(\"q4_start\")).over(\n",
    "                Window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"running_max_q4_end\",\n",
    "            F.max(F.col(\"q4_end\")).over(\n",
    "                Window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"running_max_q4_filed\",\n",
    "            F.max(F.col(\"q4_filed\")).over(\n",
    "                Window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"time_period_months_q4\",\n",
    "            F.round(\n",
    "                F.months_between(\n",
    "                    F.col(\"running_max_q4_end\"), F.col(\"running_max_q4_start\")\n",
    "                ),\n",
    "                2,\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"time_period_months_q4\",\n",
    "            F.round(\n",
    "                F.months_between(\n",
    "                    F.col(\"running_max_q4_end\"), F.col(\"running_max_q4_start\")\n",
    "                ),\n",
    "                2,\n",
    "            ),\n",
    "        )\n",
    "        .filter(\" time_period_months_q4 between 2.5 and 4 \")\n",
    "        .withColumn(\"fp\", F.lit(\"Q4\"))\n",
    "        .selectExpr(\n",
    "            \"running_max_q4_start as start\",\n",
    "            \"running_max_q4_end as end\",\n",
    "            \"fp\",\n",
    "            \"q4_rev as val\",\n",
    "            \"running_max_q4_filed as filed\",\n",
    "        )\n",
    "        .distinct()\n",
    "    )\n",
    "    return q4_rows_df\n",
    "\n",
    "# @debug\n",
    "def get_fy_all_q_union_df_pyspark(stmnts_combined_df_with_rel: DataFrame, latest_q1_q3: DataFrame, q4_rows_df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        stmnts_combined_df_with_rel.select(\"start\", \"end\", \"fp\", \"val\", \"filed\")\n",
    "        .unionByName(q4_rows_df.select(\"start\", \"end\", \"fp\", \"val\", \"filed\"))\n",
    "        .unionByName(latest_q1_q3.select(\"start\", \"end\", \"fp\", \"val\", \"filed\"))\n",
    "    )\n",
    "\n",
    "# @debug\n",
    "def period_corrected_pyspark(fy_all_q_union_df: DataFrame) -> DataFrame:\n",
    "    fy_all_q_union_df_fy_rows = fy_all_q_union_df.filter(\n",
    "        F.col(\"fp\") == \"FY\"\n",
    "    ).selectExpr([\"start as fy_start\", \"end as fy_end\"])\n",
    "    fy_all_q_union_latest_q_rows = save_latest_q1_q3(fy_all_q_union_df)\n",
    "    upto_latest_fy_pc_df = (\n",
    "        fy_all_q_union_df.crossJoin(fy_all_q_union_df_fy_rows)\n",
    "        .filter(\" start >= fy_start and end <= fy_end\")\n",
    "        .withColumn(\n",
    "            \"Period\",\n",
    "            F.when(\n",
    "                F.col(\"fp\").isin([\"FY\"]),\n",
    "                F.concat_ws(\n",
    "                    \"\",\n",
    "                    F.col(\"fp\"),\n",
    "                    F.regexp_extract(\n",
    "                        F.year(F.date_trunc(\"year\", F.col(\"fy_end\"))).cast(\n",
    "                            StringType()\n",
    "                        ),\n",
    "                        \"^\\\\d{2}(\\\\d{2})$\",\n",
    "                        1,\n",
    "                    ),\n",
    "                ),\n",
    "            ).otherwise(\n",
    "                F.concat_ws(\n",
    "                    \"\",\n",
    "                    F.reverse(F.col(\"fp\")),\n",
    "                    F.regexp_extract(\n",
    "                        F.year(F.date_trunc(\"year\", F.col(\"fy_end\"))).cast(\n",
    "                            StringType()\n",
    "                        ),\n",
    "                        \"^\\\\d{2}(\\\\d{2})$\",\n",
    "                        1,\n",
    "                    ),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        .select(\"start\", \"end\", \"fp\", \"Period\", \"val\", \"filed\")\n",
    "    )\n",
    "    max_end_fy = fy_all_q_union_df_fy_rows.selectExpr(\" max(fy_end) as max_end_fy \")\n",
    "    try:\n",
    "        period_val_max_end_fy = (\n",
    "            upto_latest_fy_pc_df.join(\n",
    "                max_end_fy, on=F.col(\"end\") == F.col(\"max_end_fy\"), how=\"inner\"\n",
    "            )\n",
    "            .selectExpr(\"regexp_extract(Period, r'^.*(\\\\d{2,}).*$', 1)\")\n",
    "            .first()[0]\n",
    "        )\n",
    "        period_for_latest_q_rows = str(int(period_val_max_end_fy) + 1)\n",
    "        fy_all_q_union_latest_q_pc_rows = fy_all_q_union_latest_q_rows.withColumn(\n",
    "            \"Period\",\n",
    "            F.concat_ws(\"\", F.reverse(F.col(\"fp\")), F.lit(period_for_latest_q_rows)),\n",
    "        ).select(\"start\", \"end\", \"fp\", \"Period\", \"val\", \"filed\")\n",
    "    except TypeError:\n",
    "        return upto_latest_fy_pc_df\n",
    "    return upto_latest_fy_pc_df.unionByName(fy_all_q_union_latest_q_pc_rows)\n",
    "\n",
    "# @debug\n",
    "def get_base_sheet_format_pyspark(fy_all_q_union_df: DataFrame, Ticker: str) -> DataFrame:\n",
    "    # base sheet format\n",
    "    fy_all_q_union_bsf_df = (\n",
    "        fy_all_q_union_df.withColumn(\"Ticker\", F.lit(f\"{Ticker}\" + \" \" + \"US Equity\"))\n",
    "        .withColumn(\"Period_Startdate\", F.date_format(F.col(\"start\"), \"M/d/y\"))\n",
    "        .withColumn(\"Period_Reportdate\", F.date_format(F.col(\"filed\"), \"M/d/y\"))\n",
    "        .withColumn(\"Period_Enddate\", F.date_format(F.col(\"end\"), \"M/d/y\"))\n",
    "        .withColumn(\"Value\", F.format_number(F.col(\"val\"), \"###,###\"))\n",
    "        .withColumn(\"KPI\", F.lit(\"rev_Topline\"))\n",
    "        .withColumn(\"_\", F.lit(\"\"))\n",
    "        .withColumn(\"Segment_Identifier\", F.lit(\"\"))\n",
    "        .withColumn(\"Revison_Date\", F.lit(\"\"))\n",
    "        .withColumn(\"Original_Value\", F.lit(\"\"))\n",
    "        .withColumn(\n",
    "            \"Source\",\n",
    "            F.when(\n",
    "                F.col(\"fp\").isin([\"FY\", \"Q4\"]),\n",
    "                F.concat_ws(\" \", F.lit(\"10-K\"), F.col(\"Period\")),\n",
    "            ).otherwise(F.concat_ws(\" \", F.lit(\"10-Q\"), F.col(\"Period\"))),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fy_all_q_union_bsf_df = fy_all_q_union_bsf_df.select(\n",
    "        [\n",
    "            \"Ticker\",\n",
    "            \"Period\",\n",
    "            \"Period_Startdate\",\n",
    "            \"Period_Enddate\",\n",
    "            \"Period_Reportdate\",\n",
    "            \"_\",\n",
    "            \"Segment_Identifier\",\n",
    "            \"KPI\",\n",
    "            \"Value\",\n",
    "            \"Revison_Date\",\n",
    "            \"Original_Value\",\n",
    "            \"Source\",\n",
    "        ]\n",
    "    ).filter((F.col(\"Period\").rlike(\"Q|F\")) & (F.col(\"val\") > 0))\n",
    "    return fy_all_q_union_bsf_df\n",
    "\n",
    "# @debug\n",
    "def get_not_in_base_sheet_rows_pyspark(fy_all_q_union_df: DataFrame, Ticker: str) -> DataFrame:\n",
    "    not_in_base_sheet_df = (\n",
    "        F.broadcast(\n",
    "            fy_all_q_union_df.withColumn(\n",
    "                \"period_sub_str\", F.col(\"period\").substr(0, 2)\n",
    "            ).alias(\"a\")\n",
    "        )\n",
    "        .join(\n",
    "            table(\"bronze.internal_products.financials_cians_raw\")\n",
    "            .filter(F.col(\"ticker\") == Ticker + \" US Equity\")\n",
    "            .filter(F.col(\"kpi\") == \"rev_Topline\")\n",
    "            .filter(F.col(\"value\").isNotNull())\n",
    "            .select(\"period\", \"period_startdate\", \"period_enddate\")\n",
    "            .withColumn(\"period_sub_str\", F.col(\"period\").substr(0, 2))\n",
    "            .alias(\"b\"),\n",
    "            on=(F.col(\"a.Period_Startdate\") == F.col(\"b.period_startdate\"))\n",
    "            & (F.col(\"a.Period_Enddate\") == F.col(\"b.period_enddate\"))\n",
    "            & (F.col(\"a.period_sub_str\") == F.col(\"b.period_sub_str\")),\n",
    "            how=\"left_anti\",\n",
    "        )\n",
    "        .drop(\"period_sub_str\")\n",
    "    )\n",
    "    return not_in_base_sheet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd078eb5-2704-4e7f-95b8-08530ffe6498",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pandas functions"
    }
   },
   "outputs": [],
   "source": [
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def get_input_tickers(tickers_str_with_region:str) -> list[str]:\n",
    "    # segregate tickers by region\n",
    "    tickers_list_us_region = [\n",
    "        ticker.lstrip().rstrip()\n",
    "        for ticker in tickers_str_with_region.split(\"\\n\")\n",
    "        if ticker != \"\" and ticker.endswith(\"-US\")\n",
    "    ]\n",
    "    # print(tickers_list_us_region)\n",
    "    tickers_list_int_region = [\n",
    "        ticker.lstrip().rstrip()\n",
    "        for ticker in tickers_str_with_region.split(\"\\n\")\n",
    "        if ticker != \"\" and not ticker.endswith(\"-US\")\n",
    "    ]\n",
    "    # print(tickers_list_int_region)\n",
    "    # get the ticker part only\n",
    "    tickers_list = [ticker.split(\"-\")[0] for ticker in tickers_list_us_region]\n",
    "\n",
    "    # handle cases with duplicate entries\n",
    "    tickers_to_check = [ticker for ticker in set(tickers_list)]\n",
    "\n",
    "    # add international tickers to flag\n",
    "    rev_bif_not_found.extend([t for t in list(set(tickers_list_int_region)) if t != \"\"])\n",
    "    # print(rev_bif_not_found)\n",
    "    return tickers_to_check\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def get_cik_lookup_df() -> pd.DataFrame:\n",
    "    # Get CIK lookup table through company tickers endpoint\n",
    "    cik_map = requests.get(\n",
    "        \"https://www.sec.gov/files/company_tickers.json\", headers=header\n",
    "    )\n",
    "    cik_map_json = cik_map.json()\n",
    "    cik_map_list = [\n",
    "        {\n",
    "            \"ticker\": v[\"ticker\"],\n",
    "            #padding to get 10 digit cik \n",
    "            \"cik\": str(v[\"cik_str\"]).zfill(10),\n",
    "            \"title\": v[\"title\"]\n",
    "        }\n",
    "        for v in cik_map_json.values()\n",
    "    ]\n",
    "    cik_map_df = pd.DataFrame(cik_map_list)\n",
    "    return cik_map_df\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def get_company_facts(cik:str) -> dict:\n",
    "    # company facts endpoint\n",
    "    URL = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    comp_facts = requests.get(URL, headers=header)\n",
    "    # time.sleep(1)\n",
    "    comp_facts_dict = json.loads(comp_facts.text)\n",
    "    return comp_facts_dict\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def create_stmnts_combined_df(comp_facts_dict: dict, history_years: int= 3) -> pd.DataFrame | None:\n",
    "    # stmnts_to_scan= [stmnt for stmnt in comp_facts_dict['facts']['us-gaap'].keys() if re.search(\"revenue\", stmnt.lower())]\n",
    "    if \"us-gaap\" not in comp_facts_dict[\"facts\"].keys():\n",
    "        return None\n",
    "    else:\n",
    "        stmnts_to_scan = [\n",
    "            \"RevenueFromContractWithCustomerExcludingAssessedTax\",\n",
    "            \"RevenueFromContractWithCustomerIncludingAssessedTax\",\n",
    "            # 'RevenueFromContractsWithCustomers',\n",
    "            \"Revenues\",\n",
    "        ]\n",
    "        stmnts_combined_df_op = pd.DataFrame([], columns= [\"start\", \"end\", \"fp\", \"time_period_months\", \"val\", \"filed\", \"orig_stmnt\"])\n",
    "        for stmnt in stmnts_to_scan:\n",
    "            try:\n",
    "                stmnt_df = pd.DataFrame(\n",
    "                    comp_facts_dict[\"facts\"][\"us-gaap\"][stmnt][\"units\"][\"USD\"]\n",
    "                )\n",
    "                # check must have columns -- if any statements lacks these skip it\n",
    "                if not (\n",
    "                    {\"start\", \"end\", \"fp\", \"val\", \"filed\"}.issubset(\n",
    "                        set(stmnt_df.columns)\n",
    "                    )\n",
    "                ):\n",
    "                    continue\n",
    "                # basic transformations and filtering\n",
    "                else:\n",
    "                    stmnt_df = stmnt_df.drop_duplicates(subset=[\"start\", \"end\", \"fp\", \"val\"])\n",
    "\n",
    "                    stmnt_df['start'] = pd.to_datetime(stmnt_df['start']).dt.date\n",
    "                    stmnt_df['end'] = pd.to_datetime(stmnt_df['end']).dt.date\n",
    "                    stmnt_df['filed'] = pd.to_datetime(stmnt_df['filed']).dt.date\n",
    "                    stmnt_df['val'] = stmnt_df['val'].astype(int)\n",
    "                    #for debugging\n",
    "                    stmnt_df['orig_stmnt'] = stmnt\n",
    "\n",
    "                    def get_time_period_months(row):\n",
    "                        row['time_period_months'] = (row['end'].year - row['start'].year)*12 + (row['end'].month - row['start'].month)\n",
    "                        return row\n",
    "\n",
    "                    stmnt_df = stmnt_df.apply(lambda row: get_time_period_months(row), axis= 1)\n",
    "\n",
    "                    # filtering out records that are not within the last history_years --IMPORTANT\n",
    "                    stmnt_df = stmnt_df[stmnt_df['start'] > (datetime.now() - timedelta(days=366*history_years)).date()]\n",
    "\n",
    "                    # filter out rows that are not quarterly/yearly\n",
    "                    stmnt_df = stmnt_df[stmnt_df['time_period_months'].isin([2, 3, 4]) | stmnt_df['time_period_months'].isin([10, 11, 12])]\n",
    "                    stmnt_df = stmnt_df[[\"start\", \"end\", \"fp\", \"time_period_months\", \"val\", \"filed\", \"orig_stmnt\"]]   \n",
    "\n",
    "                    #union with main df\n",
    "                    stmnts_combined_df_op = pd.concat([stmnts_combined_df_op, stmnt_df], ignore_index=True)\n",
    "            except KeyError as ke:\n",
    "                # logger.error(f\"KeyError: {ke}\")\n",
    "                continue\n",
    "    return stmnts_combined_df_op\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def remove_inconsistencies(stmnts_combined_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Revenues can be net/total/segment --only getting the total ones\n",
    "    stmnts_combined_df_cleansed = stmnts_combined_df.groupby([\"start\", \"end\", \"fp\", \"filed\", \"time_period_months\"]).agg({\"val\": \"max\"}).reset_index()\n",
    "\n",
    "    # Filtering out ambigous cases and returning the df\n",
    "    return stmnts_combined_df_cleansed[~stmnts_combined_df_cleansed['fp'].isnull() & ~((stmnts_combined_df_cleansed['time_period_months'] < 10) & (stmnts_combined_df_cleansed['fp'] == \"FY\")) & ~((stmnts_combined_df_cleansed['time_period_months'] > 4) & (stmnts_combined_df_cleansed['fp'].str.startswith(\"Q\")))]\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def save_latest_q1_q3(stmnts_combined_df_cleansed: pd.DataFrame) -> pd.DataFrame:\n",
    "    # save the latest q1-q3 for union later\n",
    "    max_end_fy = (\n",
    "        stmnts_combined_df_cleansed[stmnts_combined_df_cleansed['fp'] == \"FY\"]\n",
    "        .max()[\"end\"]\n",
    "    )\n",
    "    latest_q1_q3 = stmnts_combined_df_cleansed[stmnts_combined_df_cleansed['end'] > max_end_fy].copy()\n",
    "    if not latest_q1_q3.empty:\n",
    "        latest_q1_q3.loc[:, 'q1_q3_fy_rel'] = None  \n",
    "        return latest_q1_q3\n",
    "    cols = list(stmnts_combined_df_cleansed.columns) + [\"q1_q3_fy_rel\"]\n",
    "    return pd.DataFrame(columns=cols)\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def assign_relation_between_q_and_fy(stmnts_combined_df_cleansed: pd.DataFrame) -> pd.DataFrame:\n",
    "    fy_rows = stmnts_combined_df_cleansed[stmnts_combined_df_cleansed['fp'] == \"FY\"].rename(\n",
    "        columns = {\"start\": \"fy_start\", \"end\": \"fy_end\"}\n",
    "    )[['fy_start', 'fy_end']]\n",
    "\n",
    "    stmnts_combined_df_with_rel = (\n",
    "        pd.merge(stmnts_combined_df_cleansed, fy_rows, how=\"cross\", suffixes=(\"\", \"\"))\n",
    "    )\n",
    "    stmnts_combined_df_with_rel = stmnts_combined_df_with_rel[(stmnts_combined_df_with_rel['start'] >= stmnts_combined_df_with_rel['fy_start']) & (stmnts_combined_df_with_rel['end'] <= stmnts_combined_df_with_rel['fy_end'])]\n",
    "\n",
    "    def add_relationship_col(row):\n",
    "        row['q1_q3_fy_rel'] = str(row['fy_start']) + \" \" + str(row[\"fy_end\"])\n",
    "        return row\n",
    "    \n",
    "    stmnts_combined_df_with_rel = stmnts_combined_df_with_rel.apply(lambda row: add_relationship_col(row), axis= 1)\n",
    "\n",
    "    return stmnts_combined_df_with_rel\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def get_q4_rows(stmnts_combined_df_with_rel: pd.DataFrame) -> pd.DataFrame:\n",
    "    def q4_rows(grp):\n",
    "        grp['q4_val'] = grp[(grp['fp'] == \"FY\")]['val'].values[0] - grp[~(grp['fp'] == \"FY\")]['val'].sum()\n",
    "        grp['q4_start'] = grp[grp['fp'] == \"Q3\"]['end'] + timedelta(days=1)\n",
    "        grp['q4_end'] = grp['fy_end']\n",
    "        grp['q4_filed'] = grp[grp['fp'] == \"FY\"]['filed'].max()\n",
    "        grp['q1_q3_fy_rel'] = str(grp['fy_start'].values[0]) + \" \" + str(grp[\"fy_end\"].values[0])\n",
    "        return grp\n",
    "\n",
    "    q4_rows_df = stmnts_combined_df_with_rel.groupby('q1_q3_fy_rel', group_keys=False).apply(lambda grp: q4_rows(grp))\n",
    "    q4_rows_df = q4_rows_df.dropna(subset= ['q4_start', 'q4_end', 'q4_filed'])\n",
    "    q4_rows_df = q4_rows_df[['q4_start', 'q4_end', 'q4_filed', 'q4_val', 'q1_q3_fy_rel']].drop_duplicates().rename(columns = {'q4_start': 'start', 'q4_end': 'end', 'q4_filed': 'filed', 'q4_val': 'val'})\n",
    "    q4_rows_df['fp'] = 'Q4'\n",
    "\n",
    "    return q4_rows_df\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def get_fy_all_q_union_df(stmnts_combined_df_with_rel: pd.DataFrame, latest_q1_q3: pd.DataFrame, q4_rows_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    fy_all_q_union_df = pd.concat(\n",
    "       [\n",
    "        stmnts_combined_df_with_rel[[\"start\", \"end\", \"fp\", \"val\", \"filed\", \"q1_q3_fy_rel\"]],\n",
    "        q4_rows_df[[\"start\", \"end\", \"fp\", \"val\", \"filed\", \"q1_q3_fy_rel\"]],\n",
    "        latest_q1_q3[[\"start\", \"end\", \"fp\", \"val\", \"filed\", \"q1_q3_fy_rel\"]]\n",
    "       ]\n",
    "    )\n",
    "    return fy_all_q_union_df\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding   \n",
    "def period_corrected(fy_all_q_union_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def get_period_val(row):\n",
    "        if row['q1_q3_fy_rel'] is None:\n",
    "            if row['fp'].startswith(\"Q\"):\n",
    "                fy_rows_df = fy_all_q_union_df[fy_all_q_union_df['fp'] == 'FY'].copy()\n",
    "                row['Period'] = row['fp'][::-1] + str(int(str(fy_rows_df['end'].max()).split(\"-\")[0][-2:]) + 1)\n",
    "        else:\n",
    "            if row['fp'] == \"FY\":\n",
    "                row['Period'] = row['fp'] + row['q1_q3_fy_rel'].split(\" \")[1].split(\"-\")[0][-2:]\n",
    "            elif row['fp'].startswith(\"Q\"):\n",
    "                row['Period'] = row['fp'][::-1] + row['q1_q3_fy_rel'].split(\" \")[1].split(\"-\")[0][-2:]\n",
    "        return row\n",
    "    return fy_all_q_union_df.apply(lambda row: get_period_val(row), axis=1)\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def get_base_sheet_format(fy_all_q_union_df: pd.DataFrame, Ticker: str) -> pd.DataFrame:\n",
    "    # base sheet format\n",
    "    fy_all_q_union_df['Ticker'] = Ticker + \" \" + \"US Equity\"\n",
    "    fy_all_q_union_df['Period_Startdate'] = fy_all_q_union_df['start'].apply(lambda d: f\"{d.month}/{d.day}/{d.year}\")\n",
    "    fy_all_q_union_df['Period_Enddate'] = fy_all_q_union_df['end'].apply(lambda d: f\"{d.month}/{d.day}/{d.year}\")\n",
    "    fy_all_q_union_df['Period_Reportdate'] = fy_all_q_union_df['filed'].apply(lambda d: f\"{d.month}/{d.day}/{d.year}\")\n",
    "    fy_all_q_union_df[\"Value\"] = fy_all_q_union_df['val'].apply(lambda x: \"{:,}\".format(x))\n",
    "    fy_all_q_union_df[\"KPI\"] = \"rev_Topline\"\n",
    "    fy_all_q_union_df[\"_\"] = \"\"\n",
    "    fy_all_q_union_df[\"Segment_Identifier\"] = \"\"\n",
    "    fy_all_q_union_df[\"Revison_Date\"] = \"\"\n",
    "    fy_all_q_union_df[\"Original_Value\"] = \"\"\n",
    "    fy_all_q_union_df[\"Source\"] = fy_all_q_union_df.apply(lambda row: \"10-K \" + row['Period'] if row['fp'] in [\"FY\", \"Q4\"] else \"10-Q \" + row['Period'], axis=1)\n",
    "    \n",
    "    fy_all_q_union_bsf_df = fy_all_q_union_df[[\n",
    "            \"Ticker\",\n",
    "            \"Period\",\n",
    "            \"Period_Startdate\",\n",
    "            \"Period_Enddate\",\n",
    "            \"Period_Reportdate\",\n",
    "            \"_\",\n",
    "            \"Segment_Identifier\",\n",
    "            \"KPI\",\n",
    "            \"Value\",\n",
    "            \"Revison_Date\",\n",
    "            \"Original_Value\",\n",
    "            \"Source\",\n",
    "        ]]\n",
    "    \n",
    "    # ).filter((F.col(\"Period\").rlike(\"Q|F\")) & (F.col(\"val\") > 0))\n",
    "    return fy_all_q_union_bsf_df\n",
    "\n",
    "# @InvokeCorrespondingDeco.invoke_corresponding\n",
    "def get_not_in_base_sheet_rows(fy_all_q_union_df: pd.DataFrame, Ticker: str) -> pd.DataFrame:\n",
    "    not_in_base_sheet_df = (\n",
    "        F.broadcast(\n",
    "            spark.createDataFrame(fy_all_q_union_df).withColumn(\n",
    "                \"period_sub_str\", F.col(\"period\").substr(0, 2)\n",
    "            ).alias(\"a\")\n",
    "        )\n",
    "        .join(\n",
    "            table(\"bronze.internal_products.financials_cians_raw\")\n",
    "            .filter(F.col(\"ticker\") == Ticker + \" US Equity\")\n",
    "            .filter(F.col(\"kpi\") == \"rev_Topline\")\n",
    "            .filter(F.col(\"value\").isNotNull())\n",
    "            .select(\"period\", \"period_startdate\", \"period_enddate\")\n",
    "            .withColumn(\"period_sub_str\", F.col(\"period\").substr(0, 2))\n",
    "            .alias(\"b\"),\n",
    "            on=(F.col(\"a.Period_Startdate\") == F.col(\"b.period_startdate\"))\n",
    "            & (F.col(\"a.Period_Enddate\") == F.col(\"b.period_enddate\"))\n",
    "            & (F.col(\"a.period_sub_str\") == F.col(\"b.period_sub_str\")),\n",
    "            how=\"left_anti\",\n",
    "        )\n",
    "        .drop(\"period_sub_str\")\n",
    "    )\n",
    "    return not_in_base_sheet_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad687aef-6247-4f7f-a8d3-9bb2c093d8f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check the input tickers for their cik and title"
    }
   },
   "outputs": [],
   "source": [
    "cik_map_df = get_cik_lookup_df()\n",
    "\n",
    "cik_map_df = cik_map_df[\n",
    "    cik_map_df['ticker'].isin(get_input_tickers(tickers_str_with_region))\n",
    "]\n",
    "display(cik_map_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eff1e09-c3fc-4a7f-88b3-8e02b7a87846",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main loop "
    }
   },
   "outputs": [],
   "source": [
    "for Ticker in get_input_tickers(tickers_str_with_region= tickers_str_with_region):\n",
    "    try:\n",
    "        cik_map_df_filt = cik_map_df[cik_map_df['ticker'] == Ticker]['cik']\n",
    "        cik = cik_map_df_filt.values[0]\n",
    "    except Exception:\n",
    "        logger.warning((f\"{Ticker} not present in cik map. Check cmd 8 for more info\"))\n",
    "        rev_bif_not_found.append(Ticker + \"-US\")\n",
    "        continue\n",
    "    logger.info(f\"Processing ticker {Ticker}, cik {cik}\")\n",
    "    processed_tickers.append(Ticker + \"-US\")\n",
    "    comp_facts_dict = get_company_facts(cik= cik)\n",
    "    stmnts_combined_df = create_stmnts_combined_df(comp_facts_dict= comp_facts_dict, history_years=history_years)\n",
    "    #df must not be empty and should have atleast one FY row present otherwise assume financials not found for that particular ticker\n",
    "    if  (not stmnts_combined_df.empty and bool(sum(stmnts_combined_df[stmnts_combined_df['fp'] == 'FY'].count().values))):\n",
    "        stmnts_combined_df_cleansed = remove_inconsistencies(stmnts_combined_df)\n",
    "        latest_q1_q3 = save_latest_q1_q3(stmnts_combined_df_cleansed= stmnts_combined_df_cleansed)\n",
    "        stmnts_combined_df_with_rel = assign_relation_between_q_and_fy(\n",
    "            stmnts_combined_df_cleansed= stmnts_combined_df_cleansed\n",
    "        )\n",
    "        q4_rows_df = get_q4_rows(stmnts_combined_df_with_rel= stmnts_combined_df_with_rel)\n",
    "        fy_all_q_union_df = get_fy_all_q_union_df(\n",
    "            stmnts_combined_df_with_rel= stmnts_combined_df_with_rel, latest_q1_q3= latest_q1_q3, q4_rows_df= q4_rows_df\n",
    "        )\n",
    "        fy_all_q_union_pc_df = period_corrected(fy_all_q_union_df= fy_all_q_union_df)\n",
    "        fy_all_q_union_bsf_df = get_base_sheet_format(fy_all_q_union_pc_df, Ticker)\n",
    "        if(include_historical_data):           \n",
    "            final_result_df = pd.concat([final_result_df, fy_all_q_union_bsf_df])  \n",
    "        else:\n",
    "            not_in_base_sheet_df = get_not_in_base_sheet_rows(fy_all_q_union_bsf_df, Ticker)\n",
    "            if not_in_base_sheet_df.empty:\n",
    "                pass\n",
    "            else:\n",
    "                final_result_df = pd.concat([final_result_df, not_in_base_sheet_df])     \n",
    "    else:\n",
    "        logger.info(f\"us-gaap financials not found for {Ticker}\")\n",
    "        rev_bif_not_found.append(Ticker + \"-US\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f843aa56-78ab-4e0e-98d2-8a6d28fc0cea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check rows that needs to be added in the base sheet"
    }
   },
   "outputs": [],
   "source": [
    "final_result_df['Ticker_sort'] = final_result_df['Ticker'].str.lower()\n",
    "final_result_df['Period_Enddate_sort'] = pd.to_datetime(final_result_df['Period_Enddate'])\n",
    "final_result_df['Value_sort'] = pd.to_numeric(final_result_df['Value'].str.replace(',', ''))\n",
    "\n",
    "if not final_result_df.empty:\n",
    "    display(\n",
    "        final_result_df.sort_values(\n",
    "            by=['Ticker_sort', 'Period_Enddate_sort', 'Value_sort'],\n",
    "            ascending=False\n",
    "        ).drop(\n",
    "            columns=['Ticker_sort', 'Period_Enddate_sort', 'Value_sort']\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"No data in final_result_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90453294-c7bb-4345-945c-330255c3968b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not final_result_df.empty:\n",
    "    add_to_base_sheet = [\n",
    "        str(ticker[0]).replace(\" US Equity\", \"-US\")\n",
    "        for ticker in final_result_df[['Ticker']].drop_duplicates().values]\n",
    "else:\n",
    "    add_to_base_sheet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ba9b0a-ef13-47de-b539-7fd64f8a6c22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "QA check to see if api was hit for all tickers"
    }
   },
   "outputs": [],
   "source": [
    "new_processed_tickers = [t.replace(\"-US\", \"\") for t in set(processed_tickers)]\n",
    "new_rev_bif_not_found = [t.replace(\"-US\", \"\") for t in set(rev_bif_not_found)]\n",
    "\n",
    "if set(get_input_tickers(tickers_str_with_region)) == set(new_processed_tickers):\n",
    "    logger.info(\"No anomalies\")\n",
    "else:\n",
    "    logger.warning(\n",
    "        f\"Anomalies found for {set(get_input_tickers(tickers_str_with_region)) - set(new_processed_tickers)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa2b5a6-2f4d-4c7b-bdc8-a9760d3597d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate ticker string for next run"
    }
   },
   "outputs": [],
   "source": [
    "print((\"\\n\").join(set(processed_tickers) - set(add_to_base_sheet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "500c49aa-0348-4677-bb5f-22657f17329d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exit notebook"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\n",
    "    f\"\\nFinancials for the following tickers were not found: {list(set(rev_bif_not_found))}\\nFollowing tickers were processed: {processed_tickers}\\nNew rows for these tickers can be added: {add_to_base_sheet}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c78d4044-41c0-4254-a71c-94792a19b485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(table('bronze.internal_products.financials_cians_raw').select('ticker').distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2ba7025-c739-4abe-972a-2f31a271d09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table('bronze.internal_products.financials_cians_raw').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9629916-15a4-40f6-9700-9d5ec834f50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\" select round(months_between('2024-10-01', '2024-12-31'),2) \").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "789d1be6-8a59-4b2d-9db5-b014a89e470e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final_result_df.write.mode('overwrite').saveAsTable('silver_dev.dse_investigations.sec_gov_api_rev_bench_daily_workflow_tickers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11855835-70a1-4d50-ba37-ab18a0cb8b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(table('silver_dev.dse_investigations.sec_gov_api_rev_bench_daily_workflow_tickers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9823b4e9-939b-4a99-9b06-573b21f6648f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(table('bronze.internal_products.financials_cians_raw').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5257c259-2ea1-489b-be43-9a529525dba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pandas import json_normalize\n",
    "test_df = (spark.createDataFrame(json_normalize(comp_facts_dict['facts']['us-gaap']['RevenueFromContractWithCustomerIncludingAssessedTax']['units']))\n",
    "        .withColumn('USD', F.explode(F.col('USD')))\n",
    "        .select(\"USD.*\")\n",
    "        .withColumn('start', F.col('start').cast(DateType()))\n",
    "        .withColumn('end', F.col('end').cast(DateType()))\n",
    "        .withColumn(\"time_period_months\", F.round(F.months_between(F.col('end'), F.col('start')), 2))\n",
    "        .filter(\" start > date_sub(current_date(), ) \")\n",
    "        .filter(\"\"\" time_period_months between 2.7 and 4 or time_period_months between 11 and 13  \"\"\")\n",
    "        # # .filter(F.col('time_period_months').between(11.50, 13))\n",
    "        # .dropDuplicates(['start', 'end', 'val'])\n",
    "        # .drop(*['description', 'label', 'accn', 'frame', 'fy'])\n",
    "        # .orderBy(F.col('end').desc()))\n",
    ")\n",
    "\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59a5e3a9-0af2-4dcc-8ede-a898377081eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_df.join(get_fiscal_year_quarters(test_df), ['start', 'end', 'time_period_months'], 'left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb517c88-3099-4ff2-8e58-bda21570e007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.createDataFrame(json_normalize(comp_facts_dict['facts']['us-gaap']['Revenues']['units'])).withColumn('USD', F.explode(F.col('USD'))).select(\"USD.*\").withColumn('end', F.col('end').cast(DateType())).orderBy(F.col('end').desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "633d8dc5-37ba-4047-a0cd-61602948b3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print([each for each in comp_facts_dict['facts']['us-gaap'].keys() if re.search(\"revenue\", each.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bd2bd1d-8762-4b8f-a5ac-79c6d37b18d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(comp_facts_dict['facts']['us-gaap'][key_to_use]['units']['USD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf460a0-a65e-4c54-bab7-52ebcd686a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(comp_facts_dict['facts']['us-gaap'][key_to_use]['units']['USD'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sec_gov_revenue_KPI_pull_v2_pandas",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
